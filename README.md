## This is a project of a comparative analysis of bias within few LLM models with the usage of the Hugging Face API
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview
In this project, we focus on analyzing bias among the popular LLM models such as gpt2, Mistark, Llama2.
We put the main emphasis on comparing their performance in terms of toxicity and polarity. 

- **Toxicity**: aims to quantify the toxicity of the input texts using a pretrained hate speech classification model.
- **Polarity**: returns the estimated language polarity towards and social perceptions of a demographic (e.g. gender, race, sexual orientation).


## Tech stack 
- Python
- Hugging Face API


## Authors
- [@Karol Ziarek](https://github.com/ziarekk)
- [@Wojciech Lapacz](https://github.com/WojciechL02)

## License
Please check the MIT license that is listed in this repository.